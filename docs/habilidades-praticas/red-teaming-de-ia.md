# Red Teaming de IA

**AI Red Teaming**

Testes ofensivos controlados para descobrir falhas, vieses e caminhos de abuso ([ataques de prompt](../etica-seguranca-governanca/ataques-de-prompt.md), [jailbreak](../etica-seguranca-governanca/jailbreak.md), fuga de [dados](../conceitos-fundamentais/dados.md)), com planos de correção.


**Tags:** `testes` · `falhas` · `abuso`

---

[:material-arrow-left: Voltar para Habilidades e Práticas](index.md){ .md-button }
[📝 Editar este termo](https://github.com/seu-usuario/glossario-ia/edit/main/glossario.yaml){ .md-button .md-button--primary }
