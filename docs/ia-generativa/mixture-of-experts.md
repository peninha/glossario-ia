# Mistura de Especialistas

**Mixture of Experts** · *MoE*

Arquitetura em que vários "especialistas" (sub-redes) coexistem e um roteador escolhe apenas alguns para cada entrada/[token](../ia-generativa/token.md). Isso permite um [modelo](../conceitos-fundamentais/modelo.md) grande (com muitos [parâmetros](../conceitos-fundamentais/parametro.md)) rodar com um custo de computação menor.


**Tags:** [`deep-learning`](../tags.md#deep-learning) · [`training`](../tags.md#training)

---

[:material-arrow-left: Voltar para IA Generativa](index.md){ .md-button }
[📝 Editar este termo](https://github.com/seu-usuario/glossario-ia/edit/main/glossario.yaml){ .md-button .md-button--primary }
