# Mistura de Especialistas

**Mixture of Experts** · *MoE*

Arquitetura em que vários "especialistas" (sub-redes) coexistem e um roteador escolhe apenas alguns para cada entrada/[token](../ia-generativa/token.md). Isso permite um [modelo](../conceitos-fundamentais/modelo.md) grande (com muitos [parâmetros](../conceitos-fundamentais/parametro.md)) rodar com um custo de computação menor.


**Tags:** `arquitetura` · `eficiencia` · `especializacao`

---

[:material-arrow-left: Voltar para IA Generativa](index.md){ .md-button }
[📝 Editar este termo](https://github.com/seu-usuario/glossario-ia/edit/main/glossario.yaml){ .md-button .md-button--primary }
