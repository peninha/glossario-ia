# Mistura de Especialistas

**Mixture of Experts** Â· *MoE*

Arquitetura em que vÃ¡rios "especialistas" (sub-redes) coexistem e um roteador escolhe apenas alguns para cada entrada/[token](../ia-generativa/token.md). Isso permite um [modelo](../conceitos-fundamentais/modelo.md) grande (com muitos [parÃ¢metros](../conceitos-fundamentais/parametro.md)) rodar com um custo de computaÃ§Ã£o menor.


**Tags:** [`deep-learning`](../tags.md#deep-learning) Â· [`training`](../tags.md#training)

---

[:material-arrow-left: Voltar para IA Generativa](index.md){ .md-button }
[ğŸ“ Editar este termo](https://github.com/seu-usuario/glossario-ia/edit/main/glossario.yaml){ .md-button .md-button--primary }
