# Ataques de Prompt

**Prompt Injection**

Técnicas que induzem o [modelo](../conceitos-fundamentais/modelo.md) a ignorar instruções/limites e gerar saídas indesejadas.


**Tags:** `ataques` · `seguranca` · `manipulacao`

---

[:material-arrow-left: Voltar para Ética, Segurança e Governança](index.md){ .md-button }
[📝 Editar este termo](https://github.com/seu-usuario/glossario-ia/edit/main/glossario.yaml){ .md-button .md-button--primary }
